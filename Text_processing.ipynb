{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xwbyg9k4kVo3",
        "F8dERjyZSyAj",
        "0JaKqSyhS61K",
        "jdSURmOaWAmr",
        "A97qoiCcY1yW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Работа с регулярными выражениями и обработка текстов"
      ],
      "metadata": {
        "id": "uP-AHJ8eAlqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Замена римских цифр на арабские"
      ],
      "metadata": {
        "id": "xwbyg9k4kVo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def roman_to_arabic(roman):\n",
        "    # Словарь для конвертации римских цифр в арабские\n",
        "    roman_values = {\n",
        "        'I': 1,\n",
        "        'V': 5,\n",
        "        'X': 10,\n",
        "        'L': 50,\n",
        "        'C': 100,\n",
        "        'D': 500,\n",
        "        'M': 1000\n",
        "    }\n",
        "\n",
        "    result = 0\n",
        "    prev_value = 0\n",
        "\n",
        "    # Проходим по строке справа налево\n",
        "    for char in reversed(roman):\n",
        "        current_value = roman_values[char]\n",
        "        # Если текущее значение меньше предыдущего, вычитаем его\n",
        "        if current_value < prev_value:\n",
        "            result -= current_value\n",
        "        # Иначе прибавляем\n",
        "        else:\n",
        "            result += current_value\n",
        "        prev_value = current_value\n",
        "\n",
        "    return result\n",
        "\n",
        "def convert_roman_numbers(text):\n",
        "    # Регулярное выражение для поиска римских чисел\n",
        "    pattern = r'\\b(?:M{0,3})(?:D?C{0,3}|C[DM])(?:L?X{0,3}|X[LC])(?:I[VX]|V?I{0,3})\\b'\n",
        "\n",
        "    # Функция для замены найденных совпадений\n",
        "    def replace_match(match):\n",
        "        roman_num = match.group(0)\n",
        "        arabic_num = roman_to_arabic(roman_num)\n",
        "        # Проверяем, что число находится в допустимом диапазоне\n",
        "        if 1 <= arabic_num <= 3999:\n",
        "            return str(arabic_num)\n",
        "        return roman_num\n",
        "\n",
        "    # Выполняем замену\n",
        "    result = re.sub(pattern, replace_match, text)\n",
        "    return result\n",
        "\n",
        "# Тестирование\n",
        "test_cases = [\n",
        "    \"В XIV веке произошло MCCCXLV важных событий\",\n",
        "    \"MMXXI год был особенным\",\n",
        "    \"III + II = V\",\n",
        "    \"\",  # пустая строка\n",
        "    \"Текст без римских чисел\",\n",
        "    \"MMMM\",  # некорректное римское число (больше 3999)\n",
        "]\n",
        "\n",
        "for test in test_cases:\n",
        "    print(f\"Исходный текст: '{test}'\")\n",
        "    result = convert_roman_numbers(test)\n",
        "    print(f\"Результат: '{result}'\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXBKKJiMkVNT",
        "outputId": "b48db48f-72af-4850-9148-bb403377cc7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный текст: 'В XIV веке произошло MCCCXLV важных событий'\n",
            "Результат: 'В 14 веке произошло 1345 важных событий'\n",
            "\n",
            "Исходный текст: 'MMXXI год был особенным'\n",
            "Результат: '2021 год был особенным'\n",
            "\n",
            "Исходный текст: 'III + II = V'\n",
            "Результат: '3 + 2 = 5'\n",
            "\n",
            "Исходный текст: ''\n",
            "Результат: ''\n",
            "\n",
            "Исходный текст: 'Текст без римских чисел'\n",
            "Результат: 'Текст без римских чисел'\n",
            "\n",
            "Исходный текст: 'MMMM'\n",
            "Результат: 'MMMM'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Работа с датами\n",
        "Нахождение дат и прибавление 15"
      ],
      "metadata": {
        "id": "vdg97wi-m8ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def add_15_days(match):\n",
        "    # Получаем дату из совпадения\n",
        "    date_str = match.group(0)\n",
        "    # Преобразуем строку в объект datetime\n",
        "    date_obj = datetime.strptime(date_str, \"%d.%m.%Y\")\n",
        "    # Добавляем 15 дней\n",
        "    new_date = date_obj + timedelta(days=15)\n",
        "    # Преобразуем обратно в строку в нужном формате\n",
        "    return new_date.strftime(\"%d.%m.%Y\")\n",
        "\n",
        "def process_dates(text):\n",
        "    # Регулярное выражение для поиска дат\n",
        "    pattern = r'/\\b(?:0?[1-9]|[12]\\d|3[0-1])\\.(?:0?[13578]|1[02])\\.(?:\\d\\d\\d[1-9]|\\d\\d[1-9]\\d|\\d[1-9]\\d\\d|[1-9]\\d\\d\\d)\\b|\\b(?:0?[1-9]|[12]\\d|30)\\.(?:0?[469]|11)\\.(?:\\d\\d\\d[1-9]|\\d\\d[1-9]\\d|\\d[1-9]\\d\\d|[1-9]\\d\\d\\d)\\b|\\b(?:(?:0?[1-9]|1\\d|2[0-8]).(?:0?[1-9]|1[0-2]).(?:\\d\\d\\d[1-9]|\\d\\d[1-9]\\d|\\d[1-9]\\d\\d|[1-9]\\d\\d\\d)\\b)|\\b(?:29.0?2.(?:(?:(?:1[6-9]|[2-9]\\d)?(?:0[48]|[2468][048]|[13579][26])|(?:(?:16|[2468][048]|[3579][26])00))))\\b'\n",
        "\n",
        "    # Заменяем все найденные даты\n",
        "    result = re.sub(pattern, add_15_days, text)\n",
        "    return result\n",
        "\n",
        "# Пример использования\n",
        "text = \"\"\"\n",
        "Примеры дат:\n",
        "01.01.2023,\n",
        "28.02.2024,\n",
        "Встреча назначена на 15.03.2023, а дедлайн 30.03.2023.,\n",
        "День рождения 29.02.2020 (високосный год).,\n",
        "29.02.2019,\n",
        "31.02.2020,\n",
        "28.02.2020,\n",
        "Некорректная дата: 31.04.2023.,\n",
        "Конец года: 31.12.2023,\n",
        "Начало года: 01.01.2024,\n",
        "Разные даты: 28.02.2023 , 28.02.2024 , 29.02.2024.,\n",
        "Дата на стыке месяцев: 30.04.2023.,\n",
        "Дата на стыке годов: 25.12.2023.,\n",
        "01.01.0001 - первое января первого года\n",
        "00.00.0000 → нули везде\n",
        "12.03.0000 → нули только в году\n",
        "12.00.1989 → нули только в месяце\n",
        "00.03.1989 → нули только в дате\n",
        "01.01.1900 — магическая дата, на которой все падает.\n",
        "\n",
        "99.99.9999 → везде треш\n",
        "12.03.9999 → только в году\n",
        "12.99.1989 → только в месяце\n",
        "31.02.2010 — плохая дата\n",
        "12.15.2010 — плохой месяц\n",
        "35.15.2010 — оба плохие\n",
        "99.03.1989 → только в дате\n",
        "\"\"\"\n",
        "\n",
        "processed_text = process_dates(text)\n",
        "print(\"Исходный текст:\")\n",
        "print(text)\n",
        "print(\"\\nТекст после обработки:\")\n",
        "print(processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1qdAdqRkz-W",
        "outputId": "4cc26ed0-32ac-4c5c-892e-3090da5ec8a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный текст:\n",
            "\n",
            "Примеры дат:\n",
            "01.01.2023,\n",
            "28.02.2024,\n",
            "Встреча назначена на 15.03.2023, а дедлайн 30.03.2023.,\n",
            "День рождения 29.02.2020 (високосный год).,\n",
            "29.02.2019,\n",
            "31.02.2020,\n",
            "28.02.2020,\n",
            "Некорректная дата: 31.04.2023.,\n",
            "Конец года: 31.12.2023,\n",
            "Начало года: 01.01.2024,\n",
            "Разные даты: 28.02.2023 , 28.02.2024 , 29.02.2024.,\n",
            "Дата на стыке месяцев: 30.04.2023.,\n",
            "Дата на стыке годов: 25.12.2023.,\n",
            "01.01.0001 - первое января первого года\n",
            "00.00.0000 → нули везде\n",
            "12.03.0000 → нули только в году\n",
            "12.00.1989 → нули только в месяце\n",
            "00.03.1989 → нули только в дате\n",
            "01.01.1900 — магическая дата, на которой все падает.\n",
            "\n",
            "99.99.9999 → везде треш\n",
            "12.03.9999 → только в году\n",
            "12.99.1989 → только в месяце\n",
            "31.02.2010 — плохая дата\n",
            "12.15.2010 — плохой месяц\n",
            "35.15.2010 — оба плохие\n",
            "99.03.1989 → только в дате\n",
            "\n",
            "\n",
            "Текст после обработки:\n",
            "\n",
            "Примеры дат:\n",
            "16.01.2023,\n",
            "14.03.2024,\n",
            "Встреча назначена на 30.03.2023, а дедлайн 30.03.2023.,\n",
            "День рождения 15.03.2020 (високосный год).,\n",
            "29.02.2019,\n",
            "31.02.2020,\n",
            "14.03.2020,\n",
            "Некорректная дата: 31.04.2023.,\n",
            "Конец года: 31.12.2023,\n",
            "Начало года: 16.01.2024,\n",
            "Разные даты: 15.03.2023 , 14.03.2024 , 15.03.2024.,\n",
            "Дата на стыке месяцев: 15.05.2023.,\n",
            "Дата на стыке годов: 09.01.2024.,\n",
            "16.01.1 - первое января первого года\n",
            "00.00.0000 → нули везде\n",
            "12.03.0000 → нули только в году\n",
            "12.00.1989 → нули только в месяце\n",
            "00.03.1989 → нули только в дате\n",
            "16.01.1900 — магическая дата, на которой все падает.\n",
            "\n",
            "99.99.9999 → везде треш\n",
            "27.03.9999 → только в году\n",
            "12.99.1989 → только в месяце\n",
            "31.02.2010 — плохая дата\n",
            "12.15.2010 — плохой месяц\n",
            "35.15.2010 — оба плохие\n",
            "99.03.1989 → только в дате\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 задание\n",
        "создание частотного словаря после предобработки текста (токенизация, удаление стоп-слов, нормализации)"
      ],
      "metadata": {
        "id": "F8dERjyZSyAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Установка необходимых библиотек"
      ],
      "metadata": {
        "id": "Iz87WbDARGjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk pymorphy2"
      ],
      "metadata": {
        "id": "c4_fSSPIPXQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install corus pandas tqdm"
      ],
      "metadata": {
        "id": "LPke1qvCQckd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачивание датасета Lenta.ru"
      ],
      "metadata": {
        "id": "4vZ8Yz3KRJ8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEqaActEQden",
        "outputId": "93ffe419-1a22-4fbf-92d1-ae1c908a9871"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-01 19:44:36--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241101%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241101T194436Z&X-Amz-Expires=300&X-Amz-Signature=3f7ecb97033b837c277829aeefeee234a03d3e87b28c822dd134863883a2cc12&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-11-01 19:44:36--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241101%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241101T194436Z&X-Amz-Expires=300&X-Amz-Signature=3f7ecb97033b837c277829aeefeee234a03d3e87b28c822dd134863883a2cc12&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 527373240 (503M) [application/octet-stream]\n",
            "Saving to: ‘lenta-ru-news.csv.gz’\n",
            "\n",
            "lenta-ru-news.csv.g 100%[===================>] 502.94M  59.9MB/s    in 7.7s    \n",
            "\n",
            "2024-11-01 19:44:44 (65.2 MB/s) - ‘lenta-ru-news.csv.gz’ saved [527373240/527373240]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализованы:\n",
        "* Токенизация\n",
        "* Удаление пунктуации и цифр\n",
        "* Удаление стоп-слов\n",
        "* Нормализация слов\n",
        "\n"
      ],
      "metadata": {
        "id": "oFkYVxDFRNnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pymorphy2\n",
        "from collections import Counter\n",
        "import re\n",
        "from corus import load_lenta\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Загрузка стоп-слов для русского языка\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Токенизация текста\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Удаление пунктуации и цифр\n",
        "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "    tokens = [token for token in tokens if token]\n",
        "\n",
        "    # Удаление стоп-слов\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Нормализация слов\n",
        "    normalized_tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
        "\n",
        "    return normalized_tokens\n",
        "\n",
        "def process_lenta_dataset(path, max_articles=None):\n",
        "    # Загрузка датасета\n",
        "    records = load_lenta(path)\n",
        "\n",
        "    # Преобразование в список для подсчета длины\n",
        "    records_list = list(records)\n",
        "    if max_articles:\n",
        "        records_list = records_list[:max_articles]\n",
        "\n",
        "    all_tokens = []\n",
        "\n",
        "    # Обработка каждой статьи с прогресс-баром\n",
        "    for record in tqdm(records_list, desc=\"Обработка статей\"):\n",
        "        # Объединяем заголовок и текст статьи\n",
        "        full_text = f\"{record.title} {record.text}\"\n",
        "        # Обрабатываем текст\n",
        "        tokens = preprocess_text(full_text)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    return all_tokens\n",
        "\n",
        "def save_frequency_dict(word_counts, output_file):\n",
        "    df = pd.DataFrame(word_counts.items(), columns=['word', 'count'])\n",
        "    df = df.sort_values('count', ascending=False)\n",
        "\n",
        "    # Сохранение в файл\n",
        "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "def main():\n",
        "    dataset_path = 'lenta-ru-news.csv.gz'\n",
        "\n",
        "    # Количество статей для обработки (None для обработки всего датасета)\n",
        "    max_articles = 1000\n",
        "\n",
        "    try:\n",
        "        # Обработка датасета\n",
        "        print(\"Начало обработки датасета...\")\n",
        "        all_tokens = process_lenta_dataset(dataset_path, max_articles)\n",
        "\n",
        "        # Подсчет частоты слов\n",
        "        print(\"Подсчет частоты слов...\")\n",
        "        word_counts = Counter(all_tokens)\n",
        "\n",
        "        # Сохранение результатов\n",
        "        output_file = 'frequency_dict.csv'\n",
        "        save_frequency_dict(word_counts, output_file)\n",
        "\n",
        "        print(f\"Частотный словарь успешно сохранен в файл {output_file}\")\n",
        "        print(f\"Всего уникальных слов: {len(word_counts)}\")\n",
        "\n",
        "        # Вывод топ-10 самых частых слов\n",
        "        print(\"\\nТоп-10 самых частых слов:\")\n",
        "        for word, count in sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "            print(f\"{word}: {count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Произошла ошибка: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1bOKzngQT3-",
        "outputId": "815c82f6-ae89-4cea-bac2-8d3219036101"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Начало обработки датасета...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Обработка статей: 100%|██████████| 1000/1000 [00:31<00:00, 31.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Подсчет частоты слов...\n",
            "Частотный словарь успешно сохранен в файл frequency_dict.csv\n",
            "Всего уникальных слов: 18436\n",
            "\n",
            "Топ-10 самых частых слов:\n",
            "год: 1472\n",
            "который: 970\n",
            "россия: 886\n",
            "российский: 671\n",
            "декабрь: 668\n",
            "также: 555\n",
            "это: 554\n",
            "свой: 465\n",
            "слово: 443\n",
            "время: 435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обработка текстового файла и построение словаря, который также сохраняется в текстовый файл в формате (словоформа, количество_вхождений_словоформы) с сортировкой по количеству вхождений"
      ],
      "metadata": {
        "id": "v2JYB7-yR3xW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AerXnKdUPLRc"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pymorphy2\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Загрузка стоп-слов для русского языка\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "# Инициализация морфологического анализатора\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Токенизация текста\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Удаление пунктуации и цифр\n",
        "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "    tokens = [token for token in tokens if token]\n",
        "\n",
        "    # Удаление стоп-слов\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Нормализация слов\n",
        "    normalized_tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
        "\n",
        "    return normalized_tokens\n",
        "\n",
        "# Чтение входного файла\n",
        "input_file = 'input.txt'\n",
        "with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Предобработка текста\n",
        "processed_tokens = preprocess_text(text)\n",
        "\n",
        "# Подсчет частоты слов\n",
        "word_counts = Counter(processed_tokens)\n",
        "\n",
        "# Сортировка по частоте (по убыванию)\n",
        "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Запись результатов в выходной файл\n",
        "output_file = 'output.txt'\n",
        "with open(output_file, 'w', encoding='utf-8') as file:\n",
        "    for word, count in sorted_word_counts:\n",
        "        file.write(f\"{word}, {count}\\n\")\n",
        "\n",
        "print(f\"Частотный словарь сохранен в файл {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 задание\n",
        "вместо нормализации использована стемматизация"
      ],
      "metadata": {
        "id": "0JaKqSyhS61K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from collections import Counter\n",
        "import re\n",
        "from corus import load_lenta\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Загрузка стоп-слов для русского языка\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "# Инициализация стеммера\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Токенизация текста\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Удаление пунктуации и цифр\n",
        "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "    tokens = [token for token in tokens if token]\n",
        "\n",
        "    # Удаление стоп-слов\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Стемминг слов\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "def process_lenta_dataset(path, max_articles=None):\n",
        "    # Загрузка датасета\n",
        "    records = load_lenta(path)\n",
        "\n",
        "    # Преобразование в список для подсчета длины\n",
        "    records_list = list(records)\n",
        "    if max_articles:\n",
        "        records_list = records_list[:max_articles]\n",
        "\n",
        "    all_tokens = []\n",
        "\n",
        "    # Обработка каждой статьи с прогресс-баром\n",
        "    for record in tqdm(records_list, desc=\"Обработка статей\"):\n",
        "        # Объединяем заголовок и текст статьи\n",
        "        full_text = f\"{record.title} {record.text}\"\n",
        "        # Обрабатываем текст\n",
        "        tokens = preprocess_text(full_text)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    return all_tokens\n",
        "\n",
        "def save_frequency_dict(word_counts, output_file):\n",
        "    # Создание DataFrame для удобного сохранения\n",
        "    df = pd.DataFrame(word_counts.items(), columns=['word', 'count'])\n",
        "    df = df.sort_values('count', ascending=False)\n",
        "\n",
        "    # Сохранение в файл\n",
        "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "def main():\n",
        "    # Путь к датасету Lenta.ru\n",
        "    dataset_path = 'lenta-ru-news.csv.gz'\n",
        "\n",
        "    # Количество статей для обработки (None для обработки всего датасета)\n",
        "    max_articles = 1000  # Можно изменить или установить None\n",
        "\n",
        "    try:\n",
        "        # Обработка датасета\n",
        "        print(\"Начало обработки датасета...\")\n",
        "        all_tokens = process_lenta_dataset(dataset_path, max_articles)\n",
        "\n",
        "        # Подсчет частоты слов\n",
        "        print(\"Подсчет частоты слов...\")\n",
        "        word_counts = Counter(all_tokens)\n",
        "\n",
        "        # Сохранение результатов\n",
        "        output_file = 'frequency_dict_stemmed.csv'\n",
        "        save_frequency_dict(word_counts, output_file)\n",
        "\n",
        "        print(f\"Частотный словарь успешно сохранен в файл {output_file}\")\n",
        "        print(f\"Всего уникальных слов: {len(word_counts)}\")\n",
        "\n",
        "        # Вывод топ-10 самых частых слов\n",
        "        print(\"\\nТоп-10 самых частых слов:\")\n",
        "        for word, count in sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "            print(f\"{word}: {count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Произошла ошибка: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cntpLwWVT1DJ",
        "outputId": "295db522-23a2-4c0d-9277-1dd9f5f2a074"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Начало обработки датасета...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Обработка статей: 100%|██████████| 1000/1000 [00:11<00:00, 85.57it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Подсчет частоты слов...\n",
            "Частотный словарь успешно сохранен в файл frequency_dict_stemmed.csv\n",
            "Всего уникальных слов: 18250\n",
            "\n",
            "Топ-10 самых частых слов:\n",
            "год: 1234\n",
            "котор: 970\n",
            "росс: 886\n",
            "российск: 671\n",
            "декабр: 668\n",
            "сообща: 626\n",
            "эт: 623\n",
            "такж: 555\n",
            "сво: 465\n",
            "слов: 444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 задание\n",
        "собрать корпус документов и подсчитать tf-idf после нормализации и после стемминга"
      ],
      "metadata": {
        "id": "jdSURmOaWAmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 nltk pymorphy2 scikit-learn pandas"
      ],
      "metadata": {
        "id": "EVkp7Zg0Xzqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pymorphy2\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Загрузка стоп-слов и инициализация необходимых инструментов\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "def get_wikipedia_content(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    content = soup.find(id=\"mw-content-text\")\n",
        "    paragraphs = content.find_all('p')\n",
        "    text = ' '.join([p.get_text() for p in paragraphs])\n",
        "    return text\n",
        "\n",
        "# Список URL статей\n",
        "urls = [\n",
        "    \"https://ru.wikipedia.org/wiki/Python\",\n",
        "    \"https://ru.wikipedia.org/wiki/Java\",\n",
        "    \"https://ru.wikipedia.org/wiki/C%2B%2B\",\n",
        "    \"https://ru.wikipedia.org/wiki/JavaScript\",\n",
        "    \"https://ru.wikipedia.org/wiki/Ruby\"\n",
        "]\n",
        "\n",
        "# Собираем корпус документов\n",
        "documents = []\n",
        "for url in urls:\n",
        "    print(f\"Загрузка содержимого с {url}\")\n",
        "    content = get_wikipedia_content(url)\n",
        "    documents.append(content)\n",
        "\n",
        "# Вариант 1: Нормализация с Pymorphy2\n",
        "def normalize_pymorphy(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    normalized = [morph.parse(word)[0].normal_form for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return ' '.join(normalized)\n",
        "\n",
        "# Вариант 2: Нормализация со стеммером\n",
        "def normalize_stemmer(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stemmed = [stemmer.stem(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return ' '.join(stemmed)\n",
        "\n",
        "# Нормализация документов\n",
        "documents_pymorphy = [normalize_pymorphy(doc) for doc in documents]\n",
        "documents_stemmer = [normalize_stemmer(doc) for doc in documents]\n",
        "\n",
        "# Функция для вычисления TF-IDF\n",
        "def calculate_tfidf(documents):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "    return df_tfidf\n",
        "\n",
        "# Вычисление TF-IDF для обоих вариантов\n",
        "tfidf_pymorphy = calculate_tfidf(documents_pymorphy)\n",
        "tfidf_stemmer = calculate_tfidf(documents_stemmer)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"\\nTF-IDF матрица (Pymorphy2):\")\n",
        "print(tfidf_pymorphy)\n",
        "\n",
        "print(\"\\nTF-IDF матрица (Stemmer):\")\n",
        "print(tfidf_stemmer)\n",
        "\n",
        "# Вывод топ-10 ключевых слов для каждого документа\n",
        "def print_top_keywords(tfidf_matrix, urls):\n",
        "    for i, url in enumerate(urls):\n",
        "        print(f\"\\nТоп-10 ключевых слов для документа {url}:\")\n",
        "        top_indices = tfidf_matrix.iloc[i].nlargest(10).index\n",
        "        print(\", \".join(top_indices))\n",
        "\n",
        "print(\"\\nКлючевые слова (Pymorphy2):\")\n",
        "print_top_keywords(tfidf_pymorphy, urls)\n",
        "\n",
        "print(\"\\nКлючевые слова (Stemmer):\")\n",
        "print_top_keywords(tfidf_stemmer, urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liaM2PoGX4YW",
        "outputId": "c2b334f1-357a-42b4-ac2d-158bc15b8379"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка содержимого с https://ru.wikipedia.org/wiki/Python\n",
            "Загрузка содержимого с https://ru.wikipedia.org/wiki/Java\n",
            "Загрузка содержимого с https://ru.wikipedia.org/wiki/C%2B%2B\n",
            "Загрузка содержимого с https://ru.wikipedia.org/wiki/JavaScript\n",
            "Загрузка содержимого с https://ru.wikipedia.org/wiki/Ruby\n",
            "\n",
            "TF-IDF матрица (Pymorphy2):\n",
            "     abap       abc       abi       abs  abstract    access   acorn  \\\n",
            "0  0.0000  0.017867  0.005956  0.000000  0.000000  0.000000  0.0000   \n",
            "1  0.0000  0.000000  0.000000  0.009607  0.019213  0.000000  0.0000   \n",
            "2  0.0000  0.000000  0.000000  0.000000  0.000000  0.007942  0.0000   \n",
            "3  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0000   \n",
            "4  0.0117  0.000000  0.000000  0.000000  0.000000  0.000000  0.0117   \n",
            "\n",
            "   activerecord   activex        ad  ...     явный      ядро      язык  \\\n",
            "0        0.0000  0.000000  0.000000  ...  0.007977  0.020132  0.317846   \n",
            "1        0.0000  0.000000  0.000000  ...  0.025735  0.000000  0.169374   \n",
            "2        0.0000  0.000000  0.007942  ...  0.005319  0.004475  0.488212   \n",
            "3        0.0000  0.009206  0.000000  ...  0.000000  0.005187  0.263205   \n",
            "4        0.0117  0.000000  0.000000  ...  0.000000  0.006592  0.306632   \n",
            "\n",
            "   языковой        ян    январь  япония  японский     яркий     ясный  \n",
            "0  0.000000  0.000000  0.009610  0.0000    0.0000  0.000000  0.000000  \n",
            "1  0.000000  0.000000  0.007751  0.0000    0.0000  0.000000  0.000000  \n",
            "2  0.006408  0.007942  0.000000  0.0000    0.0000  0.015885  0.007942  \n",
            "3  0.000000  0.000000  0.000000  0.0000    0.0000  0.000000  0.000000  \n",
            "4  0.009440  0.000000  0.000000  0.0117    0.0234  0.000000  0.000000  \n",
            "\n",
            "[5 rows x 3365 columns]\n",
            "\n",
            "TF-IDF матрица (Stemmer):\n",
            "       abap       abc       abi       abs  abstract    access     acorn  \\\n",
            "0  0.000000  0.017809  0.005936  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.009657  0.019313  0.000000  0.000000   \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.007894  0.000000   \n",
            "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "4  0.011674  0.000000  0.000000  0.000000  0.000000  0.000000  0.011674   \n",
            "\n",
            "   activerecord  activex        ad  ...      ядер       ядр      язык  \\\n",
            "0      0.000000   0.0000  0.000000  ...  0.017809  0.010033  0.316809   \n",
            "1      0.000000   0.0000  0.000000  ...  0.000000  0.000000  0.170253   \n",
            "2      0.000000   0.0000  0.007894  ...  0.000000  0.004447  0.485224   \n",
            "3      0.000000   0.0092  0.000000  ...  0.000000  0.005183  0.263018   \n",
            "4      0.011674   0.0000  0.000000  ...  0.000000  0.006577  0.305949   \n",
            "\n",
            "     языков        ян     январ      япон    японск       ярк       ясн  \n",
            "0  0.000000  0.000000  0.009579  0.000000  0.000000  0.000000  0.000000  \n",
            "1  0.000000  0.000000  0.007791  0.000000  0.000000  0.000000  0.000000  \n",
            "2  0.006369  0.007894  0.000000  0.000000  0.000000  0.015788  0.007894  \n",
            "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "4  0.009418  0.000000  0.000000  0.011674  0.023348  0.000000  0.000000  \n",
            "\n",
            "[5 rows x 3228 columns]\n",
            "\n",
            "Ключевые слова (Pymorphy2):\n",
            "\n",
            "Топ-10 ключевых слов для документа https://ru.wikipedia.org/wiki/Python:\n",
            "python, язык, код, являться, программирование, который, программа, также, библиотека, gil\n",
            "\n",
            "Топ-10 ключевых слов для документа https://ru.wikipedia.org/wiki/Java:\n",
            "java, класс, метод, объект, год, язык, тип, конструктор, который, релиз\n",
            "\n",
            "Топ-10 ключевых слов для документа https://ru.wikipedia.org/wiki/C%2B%2B:\n",
            "язык, класс, си, библиотека, тип, возможность, который, разработка, код, шаблон\n",
            "\n",
            "Топ-10 ключевых слов для документа https://ru.wikipedia.org/wiki/JavaScript:\n",
            "javascript, браузер, язык, использовать, тестирование, являться, netscape, страница, использоваться, программирование\n",
            "\n",
            "Топ-10 ключевых слов для документа https://ru.wikipedia.org/wiki/Ruby:\n",
            "ruby, язык, библиотека, класс, файл, интерпретатор, год, программа, метод, ветвь\n",
            "\n",
            "Ключевые слова (Stemmer):\n",
            "\n",
            "Топ-10 ключевых слов для документа https://ru.wikipedia.org/wiki/Python:\n",
            "python, язык, код, явля, программирован, котор, программ, такж, библиотек, использ\n",
            "\n",
            "Топ-10 ключевых слов для документа https://ru.wikipedia.org/wiki/Java:\n",
            "java, класс, метод, объект, год, язык, тип, конструктор, котор, релиз\n",
            "\n",
            "Топ-10 ключевых слов для документа https://ru.wikipedia.org/wiki/C%2B%2B:\n",
            "язык, класс, си, библиотек, тип, возможн, шаблон, котор, разработк, код\n",
            "\n",
            "Топ-10 ключевых слов для документа https://ru.wikipedia.org/wiki/JavaScript:\n",
            "javascript, браузер, язык, использ, тестирован, netscape, страниц, программирован, явля, firefox\n",
            "\n",
            "Топ-10 ключевых слов для документа https://ru.wikipedia.org/wiki/Ruby:\n",
            "ruby, язык, библиотек, класс, файл, интерпретатор, год, программ, метод, дан\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 задание\n",
        "алгоритм автоматического реферирования"
      ],
      "metadata": {
        "id": "A97qoiCcY1yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pymorphy2\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self):\n",
        "        # Инициализация необходимых компонентов\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('punkt')\n",
        "        self.stop_words = set(stopwords.words('russian'))\n",
        "        self.morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "    def normalize_word(self, word):\n",
        "        \"\"\"Нормализация слова с помощью PyMorphy2\"\"\"\n",
        "        return self.morph.parse(word)[0].normal_form\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Разбиение текста на предложения и их предварительная обработка\"\"\"\n",
        "        # Разбиение на предложения\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Обработка каждого предложения\n",
        "        processed_sentences = []\n",
        "        words_in_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Токенизация и нормализация слов\n",
        "            words = word_tokenize(sentence.lower())\n",
        "            normalized_words = []\n",
        "\n",
        "            for word in words:\n",
        "                if (word.isalpha() and  # только буквенные символы\n",
        "                    word not in self.stop_words and  # не стоп-слова\n",
        "                    len(word) > 2):  # длина больше 2 символов\n",
        "                    normalized_word = self.normalize_word(word)\n",
        "                    normalized_words.append(normalized_word)\n",
        "\n",
        "            if normalized_words:  # если остались слова после фильтрации\n",
        "                processed_sentences.append(sentence)\n",
        "                words_in_sentences.append(normalized_words)\n",
        "\n",
        "        return processed_sentences, words_in_sentences\n",
        "\n",
        "    def calculate_word_weights(self, words_in_sentences):\n",
        "        \"\"\"Подсчет весов слов с помощью TF-IDF\"\"\"\n",
        "        # Подготовка документов для TF-IDF\n",
        "        documents = [' '.join(words) for words in words_in_sentences]\n",
        "\n",
        "        # Вычисление TF-IDF\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "        # Получение словаря весов слов\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        word_weights = {}\n",
        "\n",
        "        # Средний вес слова по всем предложениям\n",
        "        tfidf_mean = np.mean(tfidf_matrix.toarray(), axis=0)\n",
        "\n",
        "        for word, weight in zip(feature_names, tfidf_mean):\n",
        "            word_weights[word] = weight\n",
        "\n",
        "        return word_weights\n",
        "\n",
        "    def calculate_sentence_weights(self, words_in_sentences, word_weights):\n",
        "        \"\"\"Подсчет весов предложений\"\"\"\n",
        "        sentence_weights = []\n",
        "\n",
        "        for words in words_in_sentences:\n",
        "            weight = sum(word_weights.get(word, 0) for word in words)\n",
        "            sentence_weights.append(weight)\n",
        "\n",
        "        return sentence_weights\n",
        "\n",
        "    def summarize(self, text, compression_ratio):\n",
        "        \"\"\"Основная функция реферирования\"\"\"\n",
        "        # Предварительная обработка текста\n",
        "        original_sentences, words_in_sentences = self.preprocess_text(text)\n",
        "\n",
        "        # Подсчет весов слов\n",
        "        word_weights = self.calculate_word_weights(words_in_sentences)\n",
        "\n",
        "        # Подсчет весов предложений\n",
        "        sentence_weights = self.calculate_sentence_weights(words_in_sentences, word_weights)\n",
        "\n",
        "        # Создание пар (индекс, вес) для сортировки\n",
        "        sentence_scores = list(enumerate(sentence_weights))\n",
        "\n",
        "        # Сортировка предложений по весу\n",
        "        sorted_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Определение количества предложений в реферате\n",
        "        n_sentences = len(original_sentences)\n",
        "        n_summary_sentences = max(1, int(n_sentences * compression_ratio))\n",
        "\n",
        "        # Отбор предложений для реферата\n",
        "        selected_indices = sorted([idx for idx, _ in sorted_sentences[:n_summary_sentences]])\n",
        "        summary_sentences = [original_sentences[idx] for idx in selected_indices]\n",
        "\n",
        "        # Формирование результатов\n",
        "        summary_text = ' '.join(summary_sentences)\n",
        "\n",
        "        # Подготовка списка ключевых слов с весами\n",
        "        keywords = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Подготовка списка предложений с весами\n",
        "        sentences_with_weights = list(zip(original_sentences, sentence_weights))\n",
        "\n",
        "        return {\n",
        "            'summary': summary_text,\n",
        "            'keywords': keywords[:10],  # топ-10 ключевых слов\n",
        "            'sentences': sentences_with_weights\n",
        "        }\n",
        "\n",
        "# Пример использования\n",
        "def main():\n",
        "    # Пример текста\n",
        "    text = \"\"\"\n",
        "    Python — высокоуровневый язык программирования общего назначения.\n",
        "    Язык ориентирован на повышение производительности разработчика и читаемости кода.\n",
        "    Синтаксис ядра Python минималистичен.\n",
        "    В то же время стандартная библиотека включает большой объём полезных функций.\n",
        "    Python поддерживает несколько парадигм программирования.\n",
        "    Основные архитектурные черты — динамическая типизация, автоматическое управление памятью.\n",
        "    Язык Python используется в качестве основного языка программирования в Google.\n",
        "    Python активно используется для создания искусственного интеллекта и машинного обучения.\n",
        "    \"\"\"\n",
        "\n",
        "    # Создание экземпляра класса\n",
        "    summarizer = TextSummarizer()\n",
        "\n",
        "    # Получение реферата с коэффициентом сжатия 0.3 (30%)\n",
        "    result = summarizer.summarize(text, 0.3)\n",
        "\n",
        "    # Вывод результатов\n",
        "    print(\"Реферат:\")\n",
        "    print(result['summary'])\n",
        "    print(\"\\nКлючевые слова с весами:\")\n",
        "    for word, weight in result['keywords']:\n",
        "        print(f\"{word}: {weight:.4f}\")\n",
        "    print(\"\\nПредложения с весами:\")\n",
        "    for sentence, weight in result['sentences']:\n",
        "        print(f\"Вес: {weight:.4f} | {sentence}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0x96AT2Y8gT",
        "outputId": "7e7e5643-8ca0-4385-b8ef-c28a5fa63284"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Реферат:\n",
            "\n",
            "    Python — высокоуровневый язык программирования общего назначения. Язык Python используется в качестве основного языка программирования в Google.\n",
            "\n",
            "Ключевые слова с весами:\n",
            "python: 0.1617\n",
            "язык: 0.1489\n",
            "программирование: 0.1245\n",
            "использоваться: 0.0802\n",
            "минималистичный: 0.0687\n",
            "синтаксис: 0.0687\n",
            "ядро: 0.0687\n",
            "несколько: 0.0638\n",
            "парадигма: 0.0638\n",
            "поддерживать: 0.0638\n",
            "\n",
            "Предложения с весами:\n",
            "Вес: 0.6147 | \n",
            "    Python — высокоуровневый язык программирования общего назначения.\n",
            "Вес: 0.4425 | Язык ориентирован на повышение производительности разработчика и читаемости кода.\n",
            "Вес: 0.3676 | Синтаксис ядра Python минималистичен.\n",
            "Вес: 0.3536 | В то же время стандартная библиотека включает большой объём полезных функций.\n",
            "Вес: 0.4776 | Python поддерживает несколько парадигм программирования.\n",
            "Вес: 0.3536 | Основные архитектурные черты — динамическая типизация, автоматическое управление памятью.\n",
            "Вес: 0.8098 | Язык Python используется в качестве основного языка программирования в Google.\n",
            "Вес: 0.5250 | Python активно используется для создания искусственного интеллекта и машинного обучения.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Автоматическое реферирование для датасета, использованного ранее"
      ],
      "metadata": {
        "id": "V27yGs04ZWYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pymorphy2\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from corus import load_lenta\n",
        "import random\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self):\n",
        "        # Инициализация необходимых компонентов\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('punkt')\n",
        "        self.stop_words = set(stopwords.words('russian'))\n",
        "        self.morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "    def normalize_word(self, word):\n",
        "        \"\"\"Нормализация слова с помощью PyMorphy2\"\"\"\n",
        "        return self.morph.parse(word)[0].normal_form\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Разбиение текста на предложения и их предварительная обработка\"\"\"\n",
        "        # Разбиение на предложения\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Обработка каждого предложения\n",
        "        processed_sentences = []\n",
        "        words_in_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Токенизация и нормализация слов\n",
        "            words = word_tokenize(sentence.lower())\n",
        "            normalized_words = []\n",
        "\n",
        "            for word in words:\n",
        "                if (word.isalpha() and  # только буквенные символы\n",
        "                    word not in self.stop_words and  # не стоп-слова\n",
        "                    len(word) > 2):  # длина больше 2 символов\n",
        "                    normalized_word = self.normalize_word(word)\n",
        "                    normalized_words.append(normalized_word)\n",
        "\n",
        "            if normalized_words:  # если остались слова после фильтрации\n",
        "                processed_sentences.append(sentence)\n",
        "                words_in_sentences.append(normalized_words)\n",
        "\n",
        "        return processed_sentences, words_in_sentences\n",
        "\n",
        "    def calculate_word_weights(self, words_in_sentences):\n",
        "        \"\"\"Подсчет весов слов с помощью TF-IDF\"\"\"\n",
        "        # Подготовка документов для TF-IDF\n",
        "        documents = [' '.join(words) for words in words_in_sentences]\n",
        "\n",
        "        # Вычисление TF-IDF\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "        # Получение словаря весов слов\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        word_weights = {}\n",
        "\n",
        "        # Средний вес слова по всем предложениям\n",
        "        tfidf_mean = np.mean(tfidf_matrix.toarray(), axis=0)\n",
        "\n",
        "        for word, weight in zip(feature_names, tfidf_mean):\n",
        "            word_weights[word] = weight\n",
        "\n",
        "        return word_weights\n",
        "\n",
        "    def calculate_sentence_weights(self, words_in_sentences, word_weights):\n",
        "        \"\"\"Подсчет весов предложений\"\"\"\n",
        "        sentence_weights = []\n",
        "\n",
        "        for words in words_in_sentences:\n",
        "            weight = sum(word_weights.get(word, 0) for word in words)\n",
        "            sentence_weights.append(weight)\n",
        "\n",
        "        return sentence_weights\n",
        "\n",
        "    def summarize(self, text, compression_ratio):\n",
        "        \"\"\"Основная функция реферирования\"\"\"\n",
        "        # Предварительная обработка текста (шаги 1-3)\n",
        "        original_sentences, words_in_sentences = self.preprocess_text(text)\n",
        "\n",
        "        if not original_sentences:  # Проверка на пустой текст\n",
        "            return {\n",
        "                'summary': '',\n",
        "                'keywords': [],\n",
        "                'sentences': []\n",
        "            }\n",
        "\n",
        "        # Подсчет весов слов (шаг 4)\n",
        "        word_weights = self.calculate_word_weights(words_in_sentences)\n",
        "\n",
        "        # Подсчет весов предложений (шаг 5)\n",
        "        sentence_weights = self.calculate_sentence_weights(words_in_sentences, word_weights)\n",
        "        #(шаги 6-7)\n",
        "        # Создание пар (индекс, вес) для сортировки\n",
        "        sentence_scores = list(enumerate(sentence_weights))\n",
        "\n",
        "        # Сортировка предложений по весу\n",
        "        sorted_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Определение количества предложений в реферате (шаг 8)\n",
        "        n_sentences = len(original_sentences)\n",
        "        n_summary_sentences = max(1, int(n_sentences * compression_ratio))\n",
        "\n",
        "        # Отбор предложений для реферата\n",
        "        selected_indices = sorted([idx for idx, _ in sorted_sentences[:n_summary_sentences]])\n",
        "        summary_sentences = [original_sentences[idx] for idx in selected_indices]\n",
        "\n",
        "        # Формирование результатов\n",
        "        summary_text = ' '.join(summary_sentences)\n",
        "\n",
        "        # Подготовка списка ключевых слов с весами\n",
        "        keywords = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Подготовка списка предложений с весами\n",
        "        sentences_with_weights = list(zip(original_sentences, sentence_weights))\n",
        "\n",
        "        return {\n",
        "            'summary': summary_text,\n",
        "            'keywords': keywords[:10],  # топ-10 ключевых слов\n",
        "            'sentences': sentences_with_weights\n",
        "        }\n",
        "\n",
        "def process_lenta_dataset(path, num_samples=5):\n",
        "    \"\"\"Обработка датасета Lenta и получение случайных статей\"\"\"\n",
        "    # Загрузка датасета\n",
        "    records = load_lenta(path)\n",
        "\n",
        "    # Преобразование итератора в список для возможности многократного использования\n",
        "    articles = list(records)\n",
        "\n",
        "    # Выбор случайных статей\n",
        "    selected_articles = random.sample(articles, num_samples)\n",
        "\n",
        "    return selected_articles\n",
        "\n",
        "def main():\n",
        "    # Путь к датасету Lenta\n",
        "    dataset_path = '/content/lenta-ru-news.csv.gz'\n",
        "\n",
        "    # Создание экземпляра класса\n",
        "    summarizer = TextSummarizer()\n",
        "\n",
        "    try:\n",
        "        # Получение случайных статей из датасета\n",
        "        articles = process_lenta_dataset(dataset_path, num_samples=5)\n",
        "\n",
        "        # Обработка каждой статьи\n",
        "        for i, article in enumerate(articles, 1):\n",
        "            print(f\"\\nСтатья {i}\")\n",
        "            print(\"Заголовок:\", article.title)\n",
        "            print(\"Дата:\", article.date)\n",
        "            print(\"Тема:\", article.topic)\n",
        "            print(\"\\nОригинальный текст:\")\n",
        "            print(article.text[:500] + \"...\" if len(article.text) > 500 else article.text)\n",
        "\n",
        "            # Получение реферата с коэффициентом сжатия 0.3 (30%)\n",
        "            result = summarizer.summarize(article.text, 0.3)\n",
        "\n",
        "            print(\"\\nРеферат:\")\n",
        "            print(result['summary'])\n",
        "\n",
        "            print(\"\\nТоп-10 ключевых слов с весами:\")\n",
        "            for word, weight in result['keywords']:\n",
        "                print(f\"{word}: {weight:.4f}\")\n",
        "\n",
        "            print(\"\\nПервые 3 предложения с весами:\")\n",
        "            for sentence, weight in list(result['sentences'])[:3]:\n",
        "                print(f\"Вес: {weight:.4f} | {sentence}\")\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при обработке датасета: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhRPfwcSY9oS",
        "outputId": "3934a9e3-9baf-4553-be83-88f880641db9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Статья 1\n",
            "Заголовок: В США начали поиск распространителя вируса Petya\n",
            "Дата: None\n",
            "Тема: Интернет и СМИ\n",
            "\n",
            "Оригинальный текст:\n",
            "Совет национальной безопасности (СНБ), ФБР и министерство внутренней безопасности США начали расследование в связи с распространением вируса-вымогателя Petya, сообщает Reuters со ссылкой на пресс-секретаря СНБ. Источник агентства сообщает, что хакерская атака не угрожает национальной безопасности США, но среди жертв вируса оказались некоторые американцы. В Министерстве внутренней безопасности США заявили, что сотрудничают с другими странами по поводу распространения вируса-шифровальщика. Министе...\n",
            "\n",
            "Реферат:\n",
            "Совет национальной безопасности (СНБ), ФБР и министерство внутренней безопасности США начали расследование в связи с распространением вируса-вымогателя Petya, сообщает Reuters со ссылкой на пресс-секретаря СНБ. Источник агентства сообщает, что хакерская атака не угрожает национальной безопасности США, но среди жертв вируса оказались некоторые американцы. Petya стал причиной сбоев в работе крупного британского рекламно-коммуникационного холдинга WPP Group, датской транспортной компании Maersk, пострадали Mondelеz International, Merck&Co, представительства Mars, Nivea и других международных компаний.\n",
            "\n",
            "Топ-10 ключевых слов с весами:\n",
            "вирус: 0.0907\n",
            "безопасность: 0.0835\n",
            "компьютер: 0.0765\n",
            "сша: 0.0658\n",
            "министерство: 0.0657\n",
            "атака: 0.0653\n",
            "жертва: 0.0653\n",
            "компания: 0.0616\n",
            "petya: 0.0616\n",
            "июнь: 0.0615\n",
            "\n",
            "Первые 3 предложения с весами:\n",
            "Вес: 0.8431 | Совет национальной безопасности (СНБ), ФБР и министерство внутренней безопасности США начали расследование в связи с распространением вируса-вымогателя Petya, сообщает Reuters со ссылкой на пресс-секретаря СНБ.\n",
            "Вес: 0.6901 | Источник агентства сообщает, что хакерская атака не угрожает национальной безопасности США, но среди жертв вируса оказались некоторые американцы.\n",
            "Вес: 0.5073 | В Министерстве внутренней безопасности США заявили, что сотрудничают с другими странами по поводу распространения вируса-шифровальщика.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Статья 2\n",
            "Заголовок: Берлин счел маловероятным урегулирование украинского конфликта в ближайшее время\n",
            "Дата: None\n",
            "Тема: Мир\n",
            "\n",
            "Оригинальный текст:\n",
            "Переговоры в Минске в «нормандском» формате не означают разрешения кризиса на Украине. Такое мнение, как передает Reuters, высказал во вторник, 10 февраля, глава МИД Германии Франк-Вальтер Штайнмайер. Переговоры по украинскому кризису, по словам министра, «были полезными», однако остаются открытыми некоторые вопросы, которые требуют разрешения. «У нас есть еще один огромный шанс завтра сделать большой первый шаг на пути к деэскалации конфликта. Но ничего не решено. Проведение одного саммита не я...\n",
            "\n",
            "Реферат:\n",
            "В свою очередь, президенты Украины и США Петр Порошенко и Барак Обама выразили надежду на прогресс в переговорах в Минске, сообщает пресс-служба украинского лидера. Надежду на успех переговоров также выразил генсек ООН Пан Ги Мун, сообщает РИА Новости со ссылкой на официального представителя генсека Фархана Хака. «Генсек надеется, что саммит в \"нормандском\" формате, также как и встреча контактной группы в Минске, позволят всем сторонам вновь посвятить себя тому, чтобы положить конец этому разрушительному конфликту при уважении суверенитета, независимости и территориальной целостности Украины», — подчеркнул Хан. Встреча лидеров России, Украины, Германии и Франции должна состояться в среду, 11 февраля, в Минске. 8 февраля президенты России, Украины и Франции — Владимир Путин, Петр Порошенко и Франсуа Олланд, — а также канцлер Германии Ангела Меркель провели телефонную конференцию по вопросу урегулирования конфликта на востоке Украины.\n",
            "\n",
            "Топ-10 ключевых слов с весами:\n",
            "переговоры: 0.0748\n",
            "украина: 0.0688\n",
            "решить: 0.0526\n",
            "февраль: 0.0488\n",
            "минск: 0.0487\n",
            "россия: 0.0472\n",
            "итог: 0.0466\n",
            "встреча: 0.0431\n",
            "лидер: 0.0426\n",
            "германия: 0.0378\n",
            "\n",
            "Первые 3 предложения с весами:\n",
            "Вес: 0.3398 | Переговоры в Минске в «нормандском» формате не означают разрешения кризиса на Украине.\n",
            "Вес: 0.2628 | Такое мнение, как передает Reuters, высказал во вторник, 10 февраля, глава МИД Германии Франк-Вальтер Штайнмайер.\n",
            "Вес: 0.3497 | Переговоры по украинскому кризису, по словам министра, «были полезными», однако остаются открытыми некоторые вопросы, которые требуют разрешения.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Статья 3\n",
            "Заголовок: В России началось создание истребителя шестого поколения\n",
            "Дата: None\n",
            "Тема: Силовые структуры\n",
            "\n",
            "Оригинальный текст:\n",
            "В России началась разработка истребителя шестого поколения. Об этом, как сообщает РИА Новости, заявил бывший главнокомандующий ВВС России генерал армии Петр Дейнекин. По его словам, новый самолет будет беспилотным. Подробности перспективного проекта Дейнекин не раскрыл. При этом бывший главком ВВС отметил, что «прыгать через поколения у нас вряд ли получится», поэтому сейчас Россия занимается разработкой истребителя пятого поколения Т-50. Ранее летчик-испытатель проекта Т-50 (ПАК ФА) Сергей Богд...\n",
            "\n",
            "Реферат:\n",
            "При этом бывший главком ВВС отметил, что «прыгать через поколения у нас вряд ли получится», поэтому сейчас Россия занимается разработкой истребителя пятого поколения Т-50. Ранее летчик-испытатель проекта Т-50 (ПАК ФА) Сергей Богдан предположил, что истребитель шестого поколения появится в России не ранее, чем через 15 лет. «Казалось бы, технологии развиваются достаточно быстро, но все равно, от истребителя четвертого поколения до поколения пятого прошло 35 лет», ─ пояснил свою оценку Богдан.\n",
            "\n",
            "Топ-10 ключевых слов с весами:\n",
            "поколение: 0.1148\n",
            "истребитель: 0.0933\n",
            "шестой: 0.0784\n",
            "россия: 0.0761\n",
            "самолёт: 0.0699\n",
            "должный: 0.0654\n",
            "перспективный: 0.0653\n",
            "беспилотный: 0.0586\n",
            "разработка: 0.0556\n",
            "дейнекин: 0.0531\n",
            "\n",
            "Первые 3 предложения с весами:\n",
            "Вес: 0.4605 | В России началась разработка истребителя шестого поколения.\n",
            "Вес: 0.3960 | Об этом, как сообщает РИА Новости, заявил бывший главнокомандующий ВВС России генерал армии Петр Дейнекин.\n",
            "Вес: 0.2128 | По его словам, новый самолет будет беспилотным.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Статья 4\n",
            "Заголовок: Астронавты с \"Атлантиса\" завершили программу выходов в открытый космос\n",
            "Дата: None\n",
            "Тема: Мир\n",
            "\n",
            "Оригинальный текст:\n",
            "Астронавты c шаттла \"Атлантис\", пристыкованного к Международной космической станции, осуществили четвертый, последний в рамках текущей экспедиции, рабочий выход в космос, сообщает Reuters. Астронавты Джерри Росс (Jerry Ross) и Ли Морин (Lee Morin) завершили монтаж перил, установку прожекторов освещения и крепление фермы сегмента \"S-Zero\", которая станет частью каркаса для дальнейшего строительства МКС. Первой задачей астронавтов была установка двухметрового перильного мостика, который поможет ра...\n",
            "\n",
            "Реферат:\n",
            "Астронавты c шаттла \"Атлантис\", пристыкованного к Международной космической станции, осуществили четвертый, последний в рамках текущей экспедиции, рабочий выход в космос, сообщает Reuters. Астронавты Джерри Росс (Jerry Ross) и Ли Морин (Lee Morin) завершили монтаж перил, установку прожекторов освещения и крепление фермы сегмента \"S-Zero\", которая станет частью каркаса для дальнейшего строительства МКС. Первой задачей астронавтов была установка двухметрового перильного мостика, который поможет работающим в космосе перемещаться между воздушным шлюзом МКС и центральным каркасом станции. Для Джерри Росса этот девятый по счету за всю карьеру выход в открытый космос стал уже вторым официальным рекордом - первый был установлен в понедельник, когда Росс восьмой раз в жизни покинул обитаемый космический аппарат.\n",
            "\n",
            "Топ-10 ключевых слов с весами:\n",
            "мкс: 0.0611\n",
            "космический: 0.0609\n",
            "станция: 0.0596\n",
            "проверка: 0.0583\n",
            "первый: 0.0572\n",
            "атлантиса: 0.0557\n",
            "астронавт: 0.0512\n",
            "сбой: 0.0482\n",
            "шаттл: 0.0478\n",
            "космос: 0.0414\n",
            "\n",
            "Первые 3 предложения с весами:\n",
            "Вес: 0.5777 | Астронавты c шаттла \"Атлантис\", пристыкованного к Международной космической станции, осуществили четвертый, последний в рамках текущей экспедиции, рабочий выход в космос, сообщает Reuters.\n",
            "Вес: 0.5994 | Астронавты Джерри Росс (Jerry Ross) и Ли Морин (Lee Morin) завершили монтаж перил, установку прожекторов освещения и крепление фермы сегмента \"S-Zero\", которая станет частью каркаса для дальнейшего строительства МКС.\n",
            "Вес: 0.6008 | Первой задачей астронавтов была установка двухметрового перильного мостика, который поможет работающим в космосе перемещаться между воздушным шлюзом МКС и центральным каркасом станции.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Статья 5\n",
            "Заголовок: Ридли Скотт доверит Бекмамбетову фильм о капитане Немо\n",
            "Дата: None\n",
            "Тема: Культура\n",
            "\n",
            "Оригинальный текст:\n",
            "Компания 20th Century Fox намерена экранизировать роман Жюля Верна \"20 тысяч лье под водой\", сообщает блог HeatVision. Продюсерами ленты выступят Ридли и Тони Скотт, а на роль режиссера прочат Тимура Бекмамбетова. Российский режиссер отметил, что ему было бы интересно поработать над этим проектом, но пока не дал своего согласия. Известно, что бюджет картины будет значительным, однако точные цифры не называются. Это не единственная экранизация знаменитого романа, которая в настоящее время находит...\n",
            "\n",
            "Реферат:\n",
            "Компания 20th Century Fox намерена экранизировать роман Жюля Верна \"20 тысяч лье под водой\", сообщает блог HeatVision. Российский режиссер отметил, что ему было бы интересно поработать над этим проектом, но пока не дал своего согласия. Совсем недавно сообщалось, что Disney реанимировал собственный проект \"20 тысяч лье под водой\". Последним крупным проектом режиссера, в котором он выступил в качестве продюсера, была фантастическая лента \"Черная молния\".\n",
            "\n",
            "Топ-10 ключевых слов с весами:\n",
            "проект: 0.0869\n",
            "режиссёр: 0.0738\n",
            "который: 0.0624\n",
            "вода: 0.0599\n",
            "лье: 0.0599\n",
            "тысяча: 0.0599\n",
            "это: 0.0595\n",
            "роман: 0.0586\n",
            "лента: 0.0578\n",
            "пока: 0.0439\n",
            "\n",
            "Первые 3 предложения с весами:\n",
            "Вес: 0.4414 | Компания 20th Century Fox намерена экранизировать роман Жюля Верна \"20 тысяч лье под водой\", сообщает блог HeatVision.\n",
            "Вес: 0.3733 | Продюсерами ленты выступят Ридли и Тони Скотт, а на роль режиссера прочат Тимура Бекмамбетова.\n",
            "Вес: 0.4287 | Российский режиссер отметил, что ему было бы интересно поработать над этим проектом, но пока не дал своего согласия.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}